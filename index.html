<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Youtian Lin</title>

    <meta name="author" content="Youtian Lin">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Youtian Lin
                </p>
                <p>I work on the problem of 4D/3D reconstruction and generation. I am a incoming Ph.D. student of Prof. <a href="https://yoyo000.github.io/index.html">Yao Yao</a>  I previously pursuing Ph.D. at <a href="https://www.hit.edu.cn">Harbin Institute of Technology</a>. Before that I received my M.S from <a href="https://english.hrbeu.edu.cn">Harbin Engineering University</a> in 2021, where I was advised by Prof. <a href="https://www.researchgate.net/profile/Jian-Guan-24">Jian Guan</a>. 
                </p>
                <p style="text-align:center">
                  <a href="mailto:linyoutian.loyot@gmail.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=VhhHLhIAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/linyoutian2">Twitter (X)</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Linyou">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/IMG_0284.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/IMG_0284.JPG" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research focuses on combining computer vision and computer graphics technology, specifically 4D/3D reconstruction and generation, neural rendering, and other techniques, to develop real-world applications. I'm also interested in applying deep learning to computer vision and image processing.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/ced-nerf.png' width="160">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://github.com/Linyou/Ced-NeRF">
        <span class="papertitle">Ced-NeRF: A Compact and Efficient Method for Dynamic Neural Radiance Fields</span>
      </a>
      <br>
      <strong>Youtian Lin</strong>
      <br>
      <em>AAAI</em>, 2024 &nbsp 
      <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
      <br>
      <a href="https://github.com/Linyou/Ced-NeRF">Project Page</a>
      <!-- /
      <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
      /
      <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
      <p></p>
      <p>
      We extend the Instant-NGP framework to support dynamic scenes, and show that it can be used to train a dynamic NeRF model that is both more compact and more efficient than prior work. 
      </p>
    </td>
  </tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <img src='images/gaussian-flow.png' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://github.com/Linyou/Ced-NeRF">
      <span class="papertitle">Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle</span>
    </a>
    <br>
    <strong>Youtian Lin</strong>
    <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Zuozhuo Dai</a>,
    <a href="https://www.researchgate.net/profile/Jian-Guan-24">Siyu Zhu</a>,
    <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=JQFnV5IAAAAJ">Yao Yao</a>
    <br>
    <em>Arxiv</em>, 2023 &nbsp 
    <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
    <br>
    <a href="https://github.com/Linyou/Ced-NeRF">Project Page</a>
    <!-- /
    <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
    /
    <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
    <p></p>
    <p>
      We propose an innovative point-based method for rapid dynamic scene reconstruction and real-time rendering from both multi-view and monocular videos, leveraging advancements in point-based 3D Gaussian Splatting (3DGS). 
    </p>
  </td>
</tr>
<td style="padding:20px;width:25%;vertical-align:middle">
  <div class="one">
    <img src='images/brdf-3dgs.png' width="160">
  </div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
  <a href="https://github.com/Linyou/Ced-NeRF">
    <span class="papertitle">Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF Decomposition and Ray Tracing</span>
  </a>
  <br>
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Jian Gao</a>,
  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Chun Gu</a>,
  <strong>Youtian Lin</strong>,
  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Hao Zhu</a>,
  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Xun Cao</a>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=JQFnV5IAAAAJ">Yao Yao</a>
  <br>
  <em>Arxiv</em>, 2023 &nbsp 
  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
  <br>
  <a href="https://github.com/Linyou/Ced-NeRF">Project Page</a>
  <!-- /
  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
  /
  <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
  <p></p>
  <p>
  We introduces a novel differentiable point-based rendering framework that utilizes 3D Gaussian points to represent a scene, allowing for material and lighting decomposition, enabling real-time relighting, ray-tracing, and editing of the 3D point cloud with improved BRDF estimation and novel view rendering results. 
  </p>
</td>
</tr>
<td style="padding:20px;width:25%;vertical-align:middle">
  <div class="one">
    <img src='images/unidream.png' width="160">
  </div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
  <a href="https://github.com/Linyou/Ced-NeRF">
    <span class="papertitle">UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation</span>
  </a>
  <br>
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Zexiang Liu</a>,
  <a href="https://scholar.google.com/citations?user=a7AMvgkAAAAJ&hl=zh-CN">Yangguang Li</a>,
  <strong>Youtian Lin</strong>
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Xin Yu</a>,
  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Sida Peng</a>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Yuan-Chen Guo</a>,
  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Yan-Pei Cao</a>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Xiaojuan Qi</a>,
  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Xiaoshui Huang</a>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Ding Liang</a>,
  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Wanli Ouyang</a>
  <br>
  <em>Arxiv</em>, 2023 &nbsp 
  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
  <br>
  <a href="https://yg256li.github.io/UniDream/">Project Page</a>
  <!-- /
  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
  /
  <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
  <p></p>
  <p>
    Use a dual-phase training process for albedo-normal aligned multi-view diffusion and reconstruction models, a progressive generation procedure for geometry and albedo-textures using Score Distillation Sample (SDS), and an innovative SDS application for finalizing Physically Based Rendering (PBR) generation with fixed albedo. 
  </p>
</td>
</tr>
<td style="padding:20px;width:25%;vertical-align:middle">
  <div class="one">
    <img src='images/earl.png' width="160">
  </div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
  <a href="https://ieeexplore.ieee.org/document/10238752">
    <span class="papertitle">EARL: An Elliptical Distribution aided Adaptive Rotation Label Assignment for Oriented Object Detection in Remote Sensing Images</span>
  </a>
  <br>
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Jian Guan</a>,
  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Mingjie Xie</a>,
  <strong>Youtian Lin</strong>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=JQFnV5IAAAAJ">Guangjun He</a>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=UeOcj28AAAAJ">Pengming Feng</a>
  <br>
  <em>IEEE TGRS</em>, 2023 &nbsp 
  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
  <br>
  <p></p>
  <p>
  Incorporating adaptive scale sampling, dynamic elliptical distribution aided sampling, and spatial distance weighting to enhance the selection of high-quality positive samples.
  </p>
</td>
</tr>
<td style="padding:20px;width:25%;vertical-align:middle">
  <div class="one">
    <img src='images/toso.png' width="160">
  </div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
  <a href="https://github.com/Linyou/Ced-NeRF">
    <span class="papertitle">TOSO: Student's-T Distribution Aided One-Stage Orientation Target Detection in Remote Sensing Images</span>
  </a>
  <br>
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Pengming Feng</a>,
  <strong>Youtian Lin</strong>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Jian Guan</a>,
  <a href="https://scholar.google.com/citations?user=a7AMvgkAAAAJ&hl=zh-CN"> Guangjun He</a>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Huifeng Shi</a>,
  <a href="https://scholar.google.com/citations?user=a7AMvgkAAAAJ&hl=zh-CN">Jonathon Chambers</a>
  <br>
  <em>ICASSP</em>, 2020 &nbsp 
  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
  <br>
  <a href="https://github.com/Linyou/Ced-NeRF">Project Page</a>
  <!-- /
  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
  /
  <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
  <p></p>
  <p>
  Utilizing a one-stage keypoint based network architecture and introducing a novel geometric transformation method to achieve orientation angle regression, along with incorporating Student's-t distribution to enhance performance
  </p>
</td>
</tr>
<td style="padding:20px;width:25%;vertical-align:middle">
  <div class="one">
    <img src='images/ienet.png' width="160">
  </div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
  <a href="https://arxiv.org/abs/1912.00969">
    <span class="papertitle">IENet: Interacting Embranchment One Stage Anchor Free Detector for Orientation Aerial Object Detection</span>
  </a>
  <br>
  <strong>Youtian Lin</strong>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Pengming Feng</a>,
  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Jian Guan</a>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=JQFnV5IAAAAJ">Wenwu Wang</a>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=UeOcj28AAAAJ">Jonathon Chambers</a>
  <br>
  <em>Arxiv</em>, 2019 &nbsp 
  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
  <br>
  <p></p>
  <p>
  We addressing the challenges of computational complexity in two-stage detectors by employing a per-pixel prediction approach with a geometric transformation, a branch interactive module, and an enhanced intersection over union (IoU) loss. 
  </p>
</td>
</tr>

  </tbody></table>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
    <tr>
      <td>
        <h2>Projects</h2>
      </td>
    </tr>
  </tbody></table>
  <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        </td>
      </tr>
    </table>
  </body>
</html>
